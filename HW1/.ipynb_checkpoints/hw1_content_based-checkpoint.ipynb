{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "898f285b",
   "metadata": {
    "id": "898f285b"
   },
   "source": [
    "# Content-based models (20 PTS)\n",
    "\n",
    "In this set, you need to implement a content-based approach to solve the ranking problem. Moreover, you should add some personalization for model as a result, to provide a list of personal recommendations for every user. Thus we need to exploit the information on user-item interactions. It could be done in two ways:\n",
    "\n",
    "**1)** Constructing content-based models for every user in the dataset\n",
    "\n",
    "**2)** Constructing user profiles\n",
    "\n",
    "To evaluate your solution, you need a new metric. As this is a ranking problem, we will use $Recall@n$. $Recall@n$ will be calculated for each user individualy. \n",
    "\n",
    "$$\n",
    "Recall_u @n = \\frac{|anime_u \\cap holdout_u|}{|holdout_u|}\n",
    "$$\n",
    "\n",
    "Holdout items here are the items our model will not see during the training. \n",
    "Each user has his/her own holdout items.\n",
    "You will need a holdout in the evaluation step.\n",
    "In this step, we predict the top $N$ recommended animes. We expect that the holdout items will be within recommended items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d01225",
   "metadata": {
    "id": "c3d01225"
   },
   "source": [
    "\n",
    "\n",
    "## Content-based models with personalization (10 PTS)\n",
    "\n",
    "In this problem you need to implement simple content-based model for each user individually in order to achieve some level of personalization. Thus your model may be considered as ensemble of the personal models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380b92a9",
   "metadata": {
    "id": "380b92a9"
   },
   "source": [
    "Here we present some default functions which are used in the code below. Do not change them. Note, that the functions below are improved versions of the functions from the seminar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0124170b",
   "metadata": {
    "id": "0124170b"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'topidx' from 'evaluation' (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/evaluation/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m topidx\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'topidx' from 'evaluation' (/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/evaluation/__init__.py)"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from evaluation import topidx\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710e80e8",
   "metadata": {
    "id": "710e80e8"
   },
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a740197d",
   "metadata": {
    "id": "a740197d"
   },
   "outputs": [],
   "source": [
    "anime_cut = (\n",
    "    pd.read_csv('anime_cut.csv')\n",
    "    .dropna() # remove items w/o description\n",
    "    # remove items with empty string as descriptions\n",
    "    .loc[lambda x: x['synopsis'].str.strip().apply(len)>0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194dd70f",
   "metadata": {
    "id": "194dd70f"
   },
   "outputs": [],
   "source": [
    "def string_ids_to_ints(line, allowed_ids):\n",
    "    '''\n",
    "    Convert text representation of ids list into python list of integers.\n",
    "    Filter out ids that are not present in allowed ids.\n",
    "    '''\n",
    "    return [int(x) for x in literal_eval(line) if int(x) in allowed_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c7902e",
   "metadata": {
    "id": "b8c7902e"
   },
   "outputs": [],
   "source": [
    "\n",
    "allowed_items = set(anime_cut['anime_id'].values)\n",
    "\n",
    "reviews_cut = (\n",
    "    pd.read_csv('reviews_cut.csv')\n",
    "    .drop_duplicates(subset=['user_id', 'anime_id'])\n",
    "    .query('anime_id in @allowed_items') # ensure review texts are present\n",
    "    .assign(# convert favorites data into lists of integer ids \n",
    "        favorites_anime = lambda x:\n",
    "            x['favorites_anime']\n",
    "            .apply(string_ids_to_ints, args=(allowed_items,))\n",
    "    )\n",
    "    .loc[lambda x: x['favorites_anime'].apply(len)>0] # drop users without favorites\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f252b",
   "metadata": {
    "id": "1c1f252b"
   },
   "source": [
    "## Getting test triplets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f4fab",
   "metadata": {
    "id": "131f4fab"
   },
   "outputs": [],
   "source": [
    "def split_train_test(reviews, n_pairs, score_cutoff=5, seed=0):\n",
    "    \"\"\"\n",
    "    Splits anime rating data into training and test sets for content-based filtering.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reviews : pandas.DataFrame\n",
    "        DataFrame containing ratings data.\n",
    "    n_pairs : int\n",
    "        The number of liked, disliked anime items to select per test user.\n",
    "    score_cutoff : int, optional\n",
    "        The cutoff threshold for item ratings. Items with ratings below this threshold are considered disliked. Default is 5.\n",
    "    seed : int, optional\n",
    "        Random seed to ensure reproducibility. Default is 0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of pandas.DataFrame\n",
    "        A tuple containing the training and test datasets.\n",
    "        - The training DataFrame contains the user ID, anime ID, and rating for each item.\n",
    "        - The test DataFrame contains triplets of liked, disliked, and favorite anime items for a subset of test users.\n",
    "\n",
    "    TL;DR\n",
    "    -----\n",
    "    Collects triplets of liked, disliked and favorite items for a subset of test users.\n",
    "    The remaining items of the selected test users are used for training CB models.\n",
    "    \"\"\"    \n",
    "    # select only users with at least 1 anime in favorites\n",
    "    subset = reviews.loc[\n",
    "        reviews[\"favorites_anime\"].apply(lambda x : len(x) >0),\n",
    "        ['user_id', 'anime_id', 'score', 'favorites_anime']\n",
    "    ]\n",
    "    valid_users = users_with_enough_data(subset, score_cutoff, n_pairs)\n",
    "    # select only valid users (i.e. with enough likes and dislikes) and shuffle\n",
    "    user_selection = subset.query('user_id in @valid_users')\n",
    "    likes_dislikes = gather_user_feedback(user_selection, score_cutoff, n_pairs, seed)\n",
    "    # extract favorites data\n",
    "    favorites = (\n",
    "        user_selection\n",
    "        .drop_duplicates(subset=['user_id'])\n",
    "        .set_index('user_id')\n",
    "        ['favorites_anime']\n",
    "    )\n",
    "    # combine likes, dislikes, and triplets into single dataframe\n",
    "    test_triplets = pd.merge(\n",
    "        likes_dislikes,\n",
    "        favorites,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how='inner'\n",
    "    )\n",
    "    # for each user, exclude test items from training\n",
    "    test_data = (\n",
    "        test_triplets\n",
    "        .eval('likes + dislikes + favorites_anime')\n",
    "        .explode()\n",
    "        .to_frame('anime_id')\n",
    "        .reset_index()\n",
    "    )\n",
    "    all_data = user_selection[['user_id', 'anime_id', 'score']]\n",
    "    train_data = pd.merge(\n",
    "        all_data,\n",
    "        test_data,\n",
    "        on=['user_id', 'anime_id'],\n",
    "        indicator=True, # test entries will be indicated as \"both\" or \"right_only\"\n",
    "        how='left', # train entries will be indicated as \"left_only\"\n",
    "    ).query('_merge == \"left_only\"') # select train entries only\n",
    "    return train_data.drop('_merge', axis=1), test_triplets.sort_index()\n",
    "\n",
    "\n",
    "def users_with_enough_data(data, score_cutoff, n_pairs):\n",
    "    '''\n",
    "    Return users that have enough positive and negative items.\n",
    "    '''\n",
    "    valid_users = (\n",
    "        (data[\"score\"] >= score_cutoff)\n",
    "        .groupby(data['user_id'])\n",
    "        .agg(total='size', n_positive='sum')\n",
    "        .assign(n_negative=lambda x: x.eval('total - n_positive'))\n",
    "        .eval('n_positive >= @n_pairs and n_negative >= @n_pairs')\n",
    "    )\n",
    "    return valid_users.loc[lambda x: x].index\n",
    "\n",
    "\n",
    "def gather_user_feedback(data, score_cutoff, n_pairs, seed):\n",
    "    '''\n",
    "    Extract fixed number of likes and dislikes per each user.\n",
    "    '''\n",
    "    likes_dislikes = (\n",
    "        data\n",
    "        # shuffle data to randomize selection of items\n",
    "        .sample(frac=1, random_state=seed)\n",
    "        # group items by positive/negative score for each user\n",
    "        .groupby(['user_id', data['score']>=score_cutoff])\n",
    "        ['anime_id'].apply(list) # make pos/neg item lists\n",
    "        .str[:n_pairs] # select fixed number of pos/neg items per user\n",
    "        .unstack('score') # pos/neg class as columns in dataframe\n",
    "    )\n",
    "    return likes_dislikes.rename(columns={False: 'dislikes', True: 'likes'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06f620a",
   "metadata": {
    "id": "a06f620a"
   },
   "source": [
    "## Prepearing the data\n",
    "\n",
    "Here we prepare the data. We divide  the original dataset into two disjoint parts so that for every user his\\her train history does not include likes and dislikes from test triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3f61f8",
   "metadata": {
    "id": "6f3f61f8"
   },
   "outputs": [],
   "source": [
    "reviews_train, test_triplets_ = split_train_test(reviews_cut, 3, score_cutoff=5, seed=0)\n",
    "test_triplets = test_triplets_[test_triplets_.index.isin(reviews_train['user_id'].unique())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a95799a",
   "metadata": {
    "id": "6a95799a"
   },
   "source": [
    "## Creating the model\n",
    "\n",
    "Let's come down to business!\n",
    "\n",
    "- Build a collection of regression-based CB models on the anime data from Lecture 2.\n",
    "\n",
    "- Pay attention that now you are asked to build $N$\n",
    "CB models for every user separately, taking only users' history into account. \n",
    "\n",
    "But if your model considers only synopsises of the animes from user history, your model may not see all the words from all the synopsises. Hence, during the evaluation, some features (words) will be omitted, which affects the model's predictions. Suggest the solution of this problem.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb942ff",
   "metadata": {
    "id": "dbb942ff"
   },
   "outputs": [],
   "source": [
    "cb_config = {\n",
    "    \"tfidf\": dict( # TfIDF Vectorizer config\n",
    "        ngram_range = (1, 1),\n",
    "        min_df=1, max_df=4,\n",
    "        strip_accents='unicode',\n",
    "        stop_words = 'english',\n",
    "        analyzer = 'word',\n",
    "        use_idf=True,\n",
    "        smooth_idf=True,\n",
    "        sublinear_tf=True\n",
    "    ),\n",
    "}\n",
    "# we also define a general representation of our dataset\n",
    "anime_description = {\n",
    "    'users': 'user_id',\n",
    "    'items': 'anime_id',\n",
    "    'favorites': 'favorites_anime',\n",
    "    'feedback' : 'score',\n",
    "    'feature_map': anime_cut.set_index('anime_id')['synopsis'],\n",
    "    'train_items': reviews_train['anime_id'].unique()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e7cd13",
   "metadata": {
    "id": "c9e7cd13"
   },
   "outputs": [],
   "source": [
    "def build_cb_model(config, trainset, trainset_description):\n",
    "    \"\"\"\n",
    "    Build a set of content-based models to recommend items to users based on their history of feedback.\n",
    "    Each user has a separate model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict\n",
    "        A dictionary containing configuration settings for the model.\n",
    "        \n",
    "    trainset : pd.DataFrame\n",
    "        A pandas DataFrame containing user-item-feedback tuples for training the model.\n",
    "        \n",
    "    trainset_description : dict\n",
    "        A dictionary containing the description of the trainset with the following keys:\n",
    "            - 'users': string\n",
    "                The name of the column containing user IDs in the trainset.\n",
    "            - 'items': string\n",
    "                The name of the column containing item IDs in the trainset.\n",
    "            - 'feedback': string\n",
    "                The name of the column containing feedback values in the trainset.\n",
    "            - 'feature_map': pd.Series\n",
    "                A Series containing item features mapped to their respective IDs.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing trained Linear Regression models and TfidfVectorizer objects \n",
    "        for each user ID in the trainset.\n",
    "    \"\"\"\n",
    "    userid = trainset_description['users']\n",
    "    itemid = trainset_description['items']\n",
    "    feedback = trainset_description['feedback']\n",
    "    feature_map = trainset_description['feature_map']\n",
    "    \n",
    "    train_data = trainset[[userid, itemid, feedback]].groupby(userid).agg(list)\n",
    "    users_dict = {}\n",
    "    for user_id, items, ratings in train_data.itertuples(name=None):\n",
    "        # we iterate over rows of `train_data` Dataframe\n",
    "        # note that by construction, `train_data`'s index encodes users IDs,\n",
    "        # and the two columns of `train_data` correspond to items and their ratings from user history\n",
    "        word_vectorizer = TfidfVectorizer(**config['tfidf'])\n",
    "        item_features = ...\n",
    "        tfidf_matrix = ...\n",
    "        reg =...\n",
    "        users_dict[user_id] = (reg, word_vectorizer)\n",
    "    return users_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2777af00",
   "metadata": {
    "id": "2777af00"
   },
   "outputs": [],
   "source": [
    "cb_params = build_cb_model(cb_config, reviews_train, anime_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e6a60",
   "metadata": {
    "id": "c52e6a60"
   },
   "source": [
    "# Generating recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e988d9d",
   "metadata": {
    "id": "8e988d9d"
   },
   "source": [
    "In order to evaluate the model you need to pass the model's recommendations into evaluation function.\n",
    "\n",
    "- To get predictions, you need to provide the model information about animes' synopsis from triplets (likes/dislikes/favorites). Moreover, you need to provide the model the rest of the anime from the catalog $anime_{cat}$ .\n",
    "\n",
    "$$\n",
    "anime_{test} = likes + dislikes + favourites + anime_{cat}\n",
    "$$\n",
    "\n",
    "- When you pass $anime_{test}$ into your model, you will get  predictions of scores. To get $anime_u$ (list of our top $N$ recommendations) you need to sort $anime_{test}$ according to predicted score and take top $N$ items with the highest scores \n",
    "\n",
    "$$\n",
    "anime_{u} = anime_{test}[sorted scores][:N]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e56722",
   "metadata": {
    "id": "a8e56722"
   },
   "outputs": [],
   "source": [
    "def cb_model_recommendations(params, training, testset, data_description, topn=10):\n",
    "    \"\"\"\n",
    "    Uses an ensemble of individual content-based models to generate recommendations for each test user.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : dict\n",
    "        A dictionary containing the regression model and word vectorizer for each user.\n",
    "    training : pandas.DataFrame\n",
    "        DataFrame containing the user ID, item ID, and rating for each item in the training dataset.\n",
    "    testset : pandas.DataFrame\n",
    "        DataFrame containing the user ID, liked items, disliked items, and favorite items for each test user.\n",
    "    data_description : dict\n",
    "        A dictionary containing metadata information for the dataset.\n",
    "        - feature_map : pandas.DataFrame\n",
    "            DataFrame containing the item ID and feature representation for each item.\n",
    "        - users : str\n",
    "            The name of the user ID column.\n",
    "        - items : str\n",
    "            The name of the anime ID column.\n",
    "        - train_items : numpy.ndarray\n",
    "            Array containing the unique item IDs from the training dataset.\n",
    "    topn : int, optional\n",
    "        The number of recommendations to generate per user. Default is 10.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Array containing the top n recommended anime IDs for each test user\n",
    "        with preserved ordering of rows corresponding to the order of users in testset.\n",
    "\n",
    "    TL;DR\n",
    "    -----\n",
    "    This function generates item recommendations for test users using a content-based model.\n",
    "    For each user in the testset, the function selects items not in the user's history and \n",
    "    combines them with the user's likes, dislikes, and favorites.\n",
    "    The function then applies the user's regression model to the feature representation of \n",
    "    these items and generates a score for each of them. The top-n scoring items are recommended to the user.\n",
    "\n",
    "    \"\"\"\n",
    "    feature_map = data_description['feature_map']\n",
    "    userid = data_description['users']\n",
    "    itemid = data_description['items']\n",
    "    user_history = training.groupby(userid)[itemid].apply(list)\n",
    "    all_items = data_description['train_items']\n",
    "    recs = []\n",
    "    for user_id, likes, dislikes, favs in testset.itertuples(name=None):\n",
    "        items_not_from_history = np.setdiff1d(all_items, user_history[user_id])\n",
    "        scoring_items = ...\n",
    "        reg, word_vectorizer = ...\n",
    "        tfidf_matrix = ...\n",
    "        user_scores = ...\n",
    "        user_recs = ...\n",
    "        recs.append(user_recs)\n",
    "    return np.array(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc56db8",
   "metadata": {
    "id": "8bc56db8"
   },
   "outputs": [],
   "source": [
    "cb_recs = cb_model_recommendations(cb_params, reviews_train, test_triplets, anime_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaa5dd8",
   "metadata": {
    "id": "bfaa5dd8"
   },
   "source": [
    "# EVALUATION AND HOLDOUT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdc498b",
   "metadata": {
    "id": "ecdc498b"
   },
   "source": [
    "## HOLDOUT\n",
    "-  Before evaluation, you should pick out $holdout$ items - the items our model will not see during the training. For this purpose, you need to sample $k$ elements from likes and favorites. You will need a holdout in the evaluation step. In this step, we predict the top $N$ recommended animes, and we expect that the holdout items will be within recommended items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96299775",
   "metadata": {
    "id": "96299775"
   },
   "source": [
    "## EVALUATION\n",
    "\n",
    "\n",
    "1) In this task you need to compute metric $Recall@n$, $Recall$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538dcf02",
   "metadata": {
    "id": "538dcf02"
   },
   "source": [
    "- In this task, you will solve the top $N$ recommendation problem. For this purpose, you need a more complex evaluation function.\n",
    "That's why we face so-called $Recall@n$. Namely,this metric takes as an input the list of personnel recommendations and holdout, and computes $Recall@n$ for each user separately. If our holdout animes are in the top $N$ recommendations for the user, the current recommendation is valid. To evaluate the models' ensemble, we need to average $Recall@n$ for all users where $animes_u$ - top  $N$ predicted animes for every user. \n",
    "\n",
    "\n",
    "$$\n",
    "Recall_u @n = \\frac{|anime_u \\cap holdout_u|}{|holdout_u|}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- To evaluate the whole model (model of models =)), you need to average personal recalls\n",
    "\n",
    "$$\n",
    "Recall = \\frac{1}{\\# users} \\sum_u Recall_u\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822327ae",
   "metadata": {
    "id": "822327ae"
   },
   "outputs": [],
   "source": [
    "def cb_model_evaluate(recs, holdout, topn=10):\n",
    "    '''\n",
    "    Evaluate the recommendation system using the recall metric.\n",
    "\n",
    "    Parameters:\n",
    "    recs (numpy.ndarray): A 2D numpy array containing the recommended items for each user.\n",
    "                          The shape of the array is (num_users, num_items).\n",
    "    holdout (pandas.Series): A pandas Series containing the ground truth for each user.\n",
    "                             The index of the series corresponds to the user IDs and the values are lists\n",
    "                             of item IDs.\n",
    "    topn (int): The number of top recommendations to consider.\n",
    "\n",
    "    Returns:\n",
    "    float: The recall score of the recommendation system.\n",
    "    ''' \n",
    "    recall = []\n",
    "    recs = recs[:, :topn]\n",
    "    for idx, user_id in enumerate(holdout.index):\n",
    "        user_recs = recs[idx]\n",
    "        user_items = holdout.loc[user_id]\n",
    "        user_recall = ...\n",
    "        recall.append(user_recall)\n",
    "    return np.mean(recall)\n",
    "\n",
    "\n",
    "def sample_holdout(test_triplets, k=3):\n",
    "    '''\n",
    "    The function picks out holdout elements.\n",
    "    It chooses likes and favorites_anime for every user,   \n",
    "    shufflle obtained dataset,\n",
    "    groups it by user_id, \n",
    "    set the size of holdout\n",
    "    \n",
    "    '''\n",
    "    #   Complete the holdout function.\n",
    "\n",
    "    holdout = (\n",
    "        ...\n",
    "    )\n",
    "    return holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5c30d",
   "metadata": {
    "id": "15c5c30d"
   },
   "outputs": [],
   "source": [
    "holdout = sample_holdout(test_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8813a6a7",
   "metadata": {
    "id": "8813a6a7"
   },
   "outputs": [],
   "source": [
    "cb_recall = cb_model_evaluate(cb_recs, holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcf9723",
   "metadata": {
    "id": "dfcf9723"
   },
   "source": [
    "# Similarity Based models (8 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbb9513",
   "metadata": {
    "id": "ffbb9513"
   },
   "source": [
    "The similarity-based approach is another attempt to personalize the content-based approach. We create so-called user profiles - the weighted sum of the TfIdf vectors of movies from user history to make user representations. Afterwards, we compute cosine similarities between user profile vectors and vectors of all the movies in the catalog.In the following part you need to improve similarity based approach presented on a lecture 2. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51673b7f",
   "metadata": {
    "id": "51673b7f"
   },
   "source": [
    "## USER PROFILE\n",
    "\n",
    "Let's have user $u_{i}$, who gave ratings $r_{i,j}$ to each anime $a_{j}$. (In our case $a_{j}$ is TfIdf representation of the current anime). So user profile vector will be the following:\n",
    "\n",
    "$$\n",
    "u_{i} = \\frac{\\sum_{j} (r_{i,j} \\times a_{j})}{\\sum_{j} r_{i,j}}\n",
    "$$\n",
    "\n",
    "In order to provide recommendations for the specific user we are going to compare an anime vector representation to user profile vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a184d2d",
   "metadata": {
    "id": "0a184d2d"
   },
   "source": [
    "\n",
    " \n",
    "**1)** Construct user profiles for users from test triplets. \n",
    "\n",
    "- In the first seminar, we used only users' positive reviews to create user profiles. Is this a good idea? Or we should use all the user's history? Comment on this question.\n",
    "\n",
    "\n",
    "**2)** Is cosine similarity the only similarity that can be chosen? Try different  [metrics](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html) and study how it affects the model performance.\n",
    "\n",
    "\n",
    "**3)** Construct the scoring function in the manner of CB-dased scoring function from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42237c6d",
   "metadata": {
    "id": "42237c6d"
   },
   "outputs": [],
   "source": [
    "def user_profiles(tf_idf, test_pairs, reviews, anime):\n",
    "    aid_index_dict, index_aid_dict = re_index(anime, 'anime_id')\n",
    "    \n",
    "    user_profile = dict()\n",
    "    for i in test_pairs.index:\n",
    "        ...\n",
    "    return user_profile\n",
    "\n",
    "def re_index(dataframe, column):\n",
    "    '''\n",
    "    Naive reindexing of data via two dictionaries:\n",
    "    item-users and user-items mapping\n",
    "    '''\n",
    "    column_uniques = dataframe[column].unique()\n",
    "    indexes = np.arange(len(column_uniques))\n",
    "    item_index_dict = dict(zip(column_uniques, indexes))\n",
    "    index_item_dict = dict(zip( indexes, column_uniques))\n",
    "    return item_index_dict, index_item_dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb405084",
   "metadata": {
    "id": "cb405084"
   },
   "outputs": [],
   "source": [
    "def build_sim_model(config, trainset, trainset_description):\n",
    "    word_vectorizer = TfidfVectorizer(**config['tfidf'])\n",
    "    tfidf_matrix = ...\n",
    "    users_profiles = ...\n",
    "\n",
    "    return word_vectorizer,  users_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7260da",
   "metadata": {
    "id": "1e7260da"
   },
   "outputs": [],
   "source": [
    "\n",
    "sim_config = {\n",
    "    \"tfidf\": dict(\n",
    "        ngram_range = (1, 1),\n",
    "        min_df=5, max_df=0.9,\n",
    "        strip_accents='unicode',\n",
    "        stop_words = 'english',\n",
    "        analyzer = 'word',\n",
    "        use_idf=1,\n",
    "        smooth_idf=1,\n",
    "        sublinear_tf=1\n",
    "    ),\n",
    "    \"reviews\" :  reviews_clean,\n",
    "    \"anime\" : anime_train,\n",
    "    'test_pairs' : test_pairs\n",
    "}\n",
    "\n",
    "\n",
    "anime_description = {\n",
    "    'feedback' : \"rating\",\n",
    "    \"items\": \"anime_id\",\n",
    "    \"item_features\": \"synopsis\",}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028038d7",
   "metadata": {
    "id": "028038d7"
   },
   "outputs": [],
   "source": [
    "params = build_sim_model(sim_config, anime_train, anime_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4866cb",
   "metadata": {
    "id": "3e4866cb"
   },
   "outputs": [],
   "source": [
    "def sim_model_scoring(params, test_pairs, anime, reviews):\n",
    "    word_vectorizer,  users_profiles = params\n",
    "    ...\n",
    "    return numerator / (n*m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c2b621",
   "metadata": {
    "id": "45c2b621",
    "outputId": "776187c3-3b9d-4e51-d85a-63daf1cd43c8"
   },
   "source": [
    "# MODELS COMPARISON (2 PTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de76abe",
   "metadata": {
    "id": "6de76abe"
   },
   "source": [
    "**1)** Compare the discussed model. Which of them works better and why? Comment on your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b393b1a",
   "metadata": {
    "id": "4b393b1a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c462293b",
   "metadata": {
    "id": "c462293b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "64cd544b7330e8e73b8689d110cc075e8c836a404445c2b82c04f3ea96ea86ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
