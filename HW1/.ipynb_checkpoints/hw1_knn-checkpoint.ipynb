{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from polara import get_movielens_data\n",
    "from polara.preprocessing.dataframes import leave_one_out, reindex\n",
    "\n",
    "from dataprep import transform_indices, verify_time_split, generate_interactions_matrix\n",
    "from evaluation import topn_recommendations, model_evaluate, downvote_seen_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement two variants of user-based KNN for the top-$n$ recommendations task when:\n",
    "1. similarity matrix is symmetric,\n",
    "2. similarity matrix is asymmetric.\n",
    "\n",
    "Recall, there's no reason for implementing row-wise weighting scheme in user-based KNN. So choose the weighting scheme wisely.\n",
    "\n",
    " In your experiments:  \n",
    "- Test your solution against both weak and strong generalization. \n",
    "  - In total you'll have 4 different experiments.\n",
    "- Follow the \"most-recent-item\" sampling strategy for constructing holdout.\n",
    "  - Explain potential issues of this scheme in relation to both weak and strong generalization.  \n",
    "- Report evaluation metrics, compare the models, and analyse the results.  \n",
    "- Use Movielens-1M data.\n",
    "\n",
    "**Note**: you can reuse some code from seminars if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_movielens_data(include_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak generalization test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data (1 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is\n",
    "- split data into training and holdout parts\n",
    "- build a new internal contiguous representation of user and item index based on the training data\n",
    "- make sure same index is used in the holdout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split most recent holdout item from each user\n",
    "training_, holdout_ = ...\n",
    "\n",
    "# check correct time splitting\n",
    "verify_time_split(training_, holdout_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex data to make contiguous index starting from 0 for user and item IDs\n",
    "training, data_index = ...\n",
    "\n",
    "# apply new index to the holdout data\n",
    "holdout = ...\n",
    "holdout = holdout.sort_values('userid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's also populate data description dictionary for convenience.\n",
    "- It allows using uniform names for users and items field.\n",
    "  - This way the code does't depend on the actual names in you dataset.\n",
    "  - So later you can easily switch to another dataset without changing the code fo the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description = dict(\n",
    "    users = data_index['users'].name,\n",
    "    items = data_index['items'].name,\n",
    "    feedback = 'rating',\n",
    "    n_users = len(data_index['users']),\n",
    "    n_items = len(data_index['items']),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously, let's also explicitly store our testset (i.e., ratings of test users excluding holdout items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid = data_description['users']\n",
    "seen_idx_mask = ...\n",
    "testset = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric case (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can consult the code from seminars or implement your own solution as long as it is fast enough.\n",
    "\n",
    "- Recall that subsampling of the neighborhood not only makes the algorithm run faster, but can also improve the results.  \n",
    "- **Make sure to implement some kind of neighborhood subsampling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_uknn_model(config, data, data_description):\n",
    "    user_item_mtx = ...\n",
    "\n",
    "    # compute similarity matrix\n",
    "    user_similarity = ...\n",
    "    return user_item_mtx, user_similarity\n",
    "\n",
    "\n",
    "def uknn_model_scoring(params, testset, testset_description):\n",
    "    # implement the scoring function to assign scores\n",
    "    # to all items for test users\n",
    "    user_item_mtx, user_similarity = params\n",
    "    # write your code for scoring, don't forget to return a dense array\n",
    "    ...\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_params = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_scores = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: recommending items from user history doesn't make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymmetric case (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Your task here is to implement user-based KNN with asymmetric similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_uknn_model_asym(config, data, data_description):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "\n",
    "def uknn_model_scoring_asym(params, testset, testset_description):\n",
    "    ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_params_asym = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_scores_asym = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Evaluation (1 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate top-$n$ recommendations for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_recs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_recs_asym = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['symmetric', 'asymmetric']\n",
    "uknn_recs = dict(zip(modes, [uknn_recs, uknn_recs_asym]))\n",
    "\n",
    "\n",
    "uknn_metrics = {}\n",
    "for mode, recs in uknn_recs.items():\n",
    "    if recs is None: continue\n",
    "    uknn_metrics[mode] = metrics = model_evaluate(recs, holdout, data_description)\n",
    "    print(\n",
    "        f'Similarity type: {mode}\\n'\\\n",
    "        'HR={:.3}, MRR={:.3}, COV={:.3}\\n'.format(*metrics)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strong generalization test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall that in the strong generalization test you work with the warm-start scenario.\n",
    "- It means that the set of test users is disjoint from the set of users in the training.\n",
    "- You're provided with the basic functions to help you perform correct splitting, but there're still a few places where your input is required. Make sure you understand the logic of data splitting in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Your task is to select a subset of users who have the most recent interactions in their history across entire dataset.\n",
    "- You will apply holdout splitting to only this subset.\n",
    "  - Think, why simply taking all users (as in weak generalization test) makes no sense in this scenario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_time(data, time_q=0.95, timeid='timestamp'):\n",
    "    '''\n",
    "    Split the input `data` DataFrame into two parts based on the timestamp, with the split point\n",
    "    being determined by the quantile value `time_q`. The function returns a tuple `(before, after)`\n",
    "    containing the two DataFrames. The `after` DataFrame contains the rows with timestamps greater\n",
    "    than or equal to the split point, while the `before` DataFrame contains the remaining rows. \n",
    "\n",
    "    Details:\n",
    "    The `quantile` method of the pandas DataFrame is used to calculate the time point (i.e., timestamp)\n",
    "    that divides the data into two parts based on the given quantile value `time_q`. Specifically,\n",
    "    the time point `split_timepoint` is calculated as the `time_q`th quantile of the values in the `timeid`\n",
    "    column of the `data` DataFrame, using the interpolation method of `nearest`. This means that\n",
    "    `split_timepoint` is the timestamp at or immediately after which `time_q` percent of the data points occur.    \n",
    "    '''\n",
    "    split_timepoint = data[timeid].quantile(q=time_q, interpolation='nearest')\n",
    "    after = data.query(f'{timeid} >= @split_timepoint') \n",
    "    before = data.drop(after.index)\n",
    "    return before, after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, you need to select a candidate subset of observations, from which you'll construct the the training, testset, and holdout datssets. Check the `split_by_time` function below and its description in the above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before, after = split_by_time(data, time_q=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now it's time to perform holdout sampling based on the obtained timepoint splitting. \n",
    "- Remember, you only sample from the test users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_part_, holdout_ = ... # your code for holdout sampling\n",
    "\n",
    "# verify correctness of time-based splitting,\n",
    "# i.e., for each test user, the holdout contains only future interactions w.r.t to testset\n",
    "verify_time_split(testset_part_, holdout_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ = ... # recall that training and testset must be disjoint by users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that `testset_part_` only contains interactions of the test users **after the timepoint**.\n",
    "- You need to combine it with the remaining histories of these users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all test users data into a single `testset_` Dataframe.\n",
    "testset_ = pd.concat(\n",
    "    [..., ...],\n",
    "    axis = 0,\n",
    "    ignore_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building internal representation of user and item index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `transform_indices` function for building a contiguous index starting from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, data_index = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before applying new index to the test data:\n",
    "  - note that the users in the `testset` must be the same as the users in the `holdout`.\n",
    "- Below is the corresponding function `align_test_by_users` that ensures these two datasets' alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_test_by_users(testset, holdout):\n",
    "    test_users = np.intersect1d(holdout['userid'].values, testset['userid'].values)\n",
    "    # only allow the same users to be present in both datasets\n",
    "    testset = testset.query('userid in @test_users').sort_values('userid')\n",
    "    holdout = holdout.query('userid in @test_users').sort_values('userid')\n",
    "    return testset, holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply new item index to test data and finalize the test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout = reindex(holdout_, data_index['items'], filter_invalid=True)\n",
    "testset = reindex(testset_, data_index['items'], filter_invalid=True)\n",
    "\n",
    "testset, holdout = align_test_by_users(testset, holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Think why we do not apply new index to users here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section you'll need to implement user-based KNN models for the warm-start scenario.\n",
    "- Think carefully which data must be generated at the build time and which data must be generated in the scoring function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric case (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_uknn_model(config, data, data_description):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "\n",
    "def uknn_model_scoring(params, testset, testset_description):\n",
    "    ...    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_params = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_scores = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymmetric case (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_uknn_model_asym(config, data, data_description):\n",
    "    \n",
    "    return ...\n",
    "\n",
    "def uknn_model_scoring_asym(params, testset, testset_description):\n",
    "\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_params_asym = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_scores_asym = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Evaluation (1 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate recommendations for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_recs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_recs_asym = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['symmetric', 'asymmetric']\n",
    "uknn_recs = dict(zip(modes, [uknn_recs, None]))\n",
    "\n",
    "\n",
    "uknn_metrics = {}\n",
    "for mode, recs in uknn_recs.items():\n",
    "    if recs is None: continue\n",
    "    uknn_metrics[mode] = metrics = model_evaluate(recs, holdout, data_description)\n",
    "    print(\n",
    "        f'Similarity type: {mode}\\n'\\\n",
    "        'HR={:.3}, MRR={:.3}, COV={:.3}\\n'.format(*metrics)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning (2 pts)\n",
    "- Try to find a neighborhood size that gives you better results.\n",
    "- Perform a simple grid-search experiment and report your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final analysis (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Provide an analysis on which model performs the best and explain why.\n",
    "2. Explain the difference in computational complexity of your models. Consider how the training and the recommendation generation differ for different models in terms of\n",
    "    - the amount of RAM,\n",
    "    - the amount of disk storage,\n",
    "    - the load on CPU.\n",
    "3. How else would you modify the model to improve either the quality of recommendations or computational performance? Describe at least one modification and its envisioned effect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "64cd544b7330e8e73b8689d110cc075e8c836a404445c2b82c04f3ea96ea86ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
