{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, diags, isspmatrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from polara import get_movielens_data\n",
    "from polara.preprocessing.dataframes import leave_one_out, reindex\n",
    "\n",
    "from dataprep import transform_indices, verify_time_split\n",
    "from evaluation import topn_recommendations, model_evaluate, downvote_seen_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_interactions_matrix(data, data_description, rebase_users=False):\n",
    "    '''\n",
    "    Convert pandas dataframe with interactions into a sparse matrix.\n",
    "    Allows reindexing user ids, which help ensure data consistency\n",
    "    at the scoring stage (assumes user ids are sorted in scoring array).\n",
    "    '''\n",
    "    n_users = data_description['n_users']\n",
    "    n_items = data_description['n_items']\n",
    "    print(n_users, n_items)\n",
    "    # get indices of observed data\n",
    "    user_idx = data[data_description['users']].values\n",
    "    if rebase_users:\n",
    "        user_idx, user_index = pd.factorize(user_idx, sort=True)\n",
    "        n_users = len(user_index)\n",
    "    item_idx = data[data_description['items']].values\n",
    "    feedback = data[data_description['feedback']].values\n",
    "    # construct rating matrix\n",
    "    print(user_idx.shape, item_idx.shape)\n",
    "    return csr_matrix((feedback, (user_idx, item_idx)), shape=(n_users, n_items))\n",
    "\n",
    "def to_numeric_id(data, field):\n",
    "    '''\n",
    "    Get new contiguous index by converting the data field\n",
    "    into categorical values.\n",
    "    '''\n",
    "    idx_data = data[field].astype(\"category\")\n",
    "    idx = idx_data.cat.codes\n",
    "    idx_map = idx_data.cat.categories.rename(field)\n",
    "    return idx, idx_map\n",
    "\n",
    "def cosine_similarity_zd(*args):\n",
    "    '''Build cosine similarity matrix with zero diagonal.'''\n",
    "    similarity = cosine_similarity(*args, dense_output=False)\n",
    "    similarity.setdiag(0)\n",
    "    similarity.eliminate_zeros()\n",
    "    return similarity.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement two variants of user-based KNN for the top-$n$ recommendations task when:\n",
    "1. similarity matrix is symmetric,\n",
    "2. similarity matrix is asymmetric.\n",
    "\n",
    "Recall, there's no reason for implementing row-wise weighting scheme in user-based KNN. So choose the weighting scheme wisely.\n",
    "\n",
    " In your experiments:  \n",
    "- Test your solution against both weak and strong generalization. \n",
    "  - In total you'll have 4 different experiments.\n",
    "- Follow the \"most-recent-item\" sampling strategy for constructing holdout.\n",
    "  - Explain potential issues of this scheme in relation to both weak and strong generalization.  \n",
    "- Report evaluation metrics, compare the models, and analyse the results.  \n",
    "- Use Movielens-1M data.\n",
    "\n",
    "**Note**: you can reuse some code from seminars if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_movielens_data(include_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak generalization test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data (1 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is\n",
    "- split data into training and holdout parts\n",
    "- build a new internal contiguous representation of user and item index based on the training data\n",
    "- make sure same index is used in the holdout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split most recent holdout item from each user\n",
    "training_, holdout_ = leave_one_out(\n",
    "    data,\n",
    "    target='timestamp',\n",
    "    sample_top=True,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# check correct time splitting\n",
    "verify_time_split(training_, holdout_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 2 invalid observations.\n"
     ]
    }
   ],
   "source": [
    "# reindex data to make contiguous index starting from 0 for user and item IDs\n",
    "training, data_index = transform_indices(training_, 'userid', 'movieid')\n",
    "\n",
    "# apply new index to the holdout data\n",
    "holdout = reindex(holdout_, data_index.values(), filter_invalid=True)\n",
    "holdout = holdout.sort_values('userid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's also populate data description dictionary for convenience.\n",
    "- It allows using uniform names for users and items field.\n",
    "  - This way the code does't depend on the actual names in you dataset.\n",
    "  - So later you can easily switch to another dataset without changing the code fo the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description = dict(\n",
    "    users = data_index['users'].name,\n",
    "    items = data_index['items'].name,\n",
    "    feedback = 'rating',\n",
    "    n_users = len(data_index['users']),\n",
    "    n_items = len(data_index['items']),\n",
    "    test_users = holdout[data_index['users'].name].values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously, let's also explicitly store our testset (i.e., ratings of test users excluding holdout items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid = data_description['users']\n",
    "seen_idx_mask = training[userid].isin(data_description['test_users'])\n",
    "testset = training[seen_idx_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric case (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can consult the code from seminars or implement your own solution as long as it is fast enough.\n",
    "\n",
    "- Recall that subsampling of the neighborhood not only makes the algorithm run faster, but can also improve the results.  \n",
    "- **Make sure to implement some kind of neighborhood subsampling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_similarity(similarity, k=100):\n",
    "    '''\n",
    "    For every row in similarity matrix, pick at most k entities\n",
    "    with the highest similarity scores. Disregard everything else.\n",
    "    '''\n",
    "    similarity = similarity.tocsr()\n",
    "    inds = similarity.indices\n",
    "    ptrs = similarity.indptr\n",
    "    data = similarity.data\n",
    "    new_ptrs = [0]\n",
    "    new_inds = []\n",
    "    new_data = []\n",
    "    for i in range(len(ptrs)-1):\n",
    "        start, stop = ptrs[i], ptrs[i+1]\n",
    "        if start < stop:\n",
    "            data_ = data[start:stop]\n",
    "            topk = min(len(data_), k)\n",
    "            idx = np.argpartition(data_, -topk)[-topk:]\n",
    "            new_data.append(data_[idx])\n",
    "            new_inds.append(inds[idx+start])\n",
    "            new_ptrs.append(new_ptrs[-1]+len(idx))\n",
    "        else:\n",
    "            new_ptrs.append(new_ptrs[-1])\n",
    "    new_data = np.concatenate(new_data)\n",
    "    new_inds = np.concatenate(new_inds)\n",
    "    truncated = csr_matrix(\n",
    "        (new_data, new_inds, new_ptrs),\n",
    "        shape=similarity.shape\n",
    "    )\n",
    "    return truncated  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_uknn_model(config, data, data_description):\n",
    "    user_item_mtx = generate_interactions_matrix(data, data_description)\n",
    "    # compute similarity matrix and normalization coefficients\n",
    "    user_similarity = truncate_similarity(\n",
    "        cosine_similarity_zd(user_item_mtx),\n",
    "        k = config['n_neighbors']\n",
    "    )\n",
    "    weighted = config['weighted']\n",
    "    return user_item_mtx, user_similarity, weighted\n",
    "\n",
    "def uknn_model_scoring(params, testset, testset_description):\n",
    "    user_item_mtx, user_similarity, weighted = params\n",
    "    test_users = testset_description['test_users']\n",
    "    \n",
    "    scores = user_similarity.dot(user_item_mtx)\n",
    "    \n",
    "    if not weighted:\n",
    "        return scores.toarray()[test_users, :]\n",
    "    \n",
    "    normalizer = user_similarity.dot(user_item_mtx.astype('bool'))\n",
    "    scores = np.nan_to_num(np.divide(scores, normalizer))\n",
    "    return np.array(scores[test_users, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 100\n",
    "\n",
    "uknn_params_uw = build_uknn_model(\n",
    "    {'weighted': False, 'n_neighbors': n_neighbors}, training, data_description\n",
    ")\n",
    "uknn_params_ew = build_uknn_model(\n",
    "    {'weighted': True, 'n_neighbors': n_neighbors}, training, data_description\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_scores_uw = uknn_model_scoring(uknn_params_uw, None, data_description)\n",
    "uknn_scores_ew = uknn_model_scoring(uknn_params_ew, None, data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "downvote_seen_items(uknn_scores_uw, testset, data_description)\n",
    "downvote_seen_items(uknn_scores_ew, testset, data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_recs_uw = topn_recommendations(uknn_scores_uw)\n",
    "uknn_recs_ew = topn_recommendations(uknn_scores_ew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: recommending items from user history doesn't make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighting mode: unweighted\n",
      "HR=0.085, MRR=0.0286, COV=0.176\n",
      "\n",
      "Weighting mode: elementwise\n",
      "HR=0.000994, MRR=0.000426, COV=0.755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modes = ['unweighted', 'elementwise']\n",
    "uknn_recs = dict(zip(modes, [uknn_recs_uw, uknn_recs_ew]))\n",
    "\n",
    "\n",
    "uknn_metrics = {}\n",
    "for mode, recs in uknn_recs.items():\n",
    "    uknn_metrics[mode] = metrics = model_evaluate(recs, holdout, data_description)\n",
    "    print(\n",
    "        f'Weighting mode: {mode}\\n'\\\n",
    "        'HR={:.3}, MRR={:.3}, COV={:.3}\\n'.format(*metrics)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymmetric case (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Your task here is to implement user-based KNN with asymmetric similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$R = KD^{-\\alpha}A $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_uknn_model_asym(config, data, data_description):\n",
    "    user_item_mtx = generate_interactions_matrix(data, data_description)\n",
    "    # compute similarity matrix and normalization coefficients\n",
    "    user_similarity = truncate_similarity(\n",
    "        cosine_similarity_zd(user_item_mtx),\n",
    "        k = config['n_neighbors']\n",
    "    )\n",
    "    \n",
    "    D = np.array(user_similarity.sum(axis=-1)).squeeze()\n",
    "    normalizer = diags(1 / D)\n",
    "    #print(user_similarity.shape, normalizer.shape)\n",
    "    user_similarity = user_similarity.dot(normalizer)\n",
    "    \n",
    "    return user_item_mtx, user_similarity\n",
    "\n",
    "\n",
    "def uknn_model_scoring_asym(params, testset, testset_description):\n",
    "    user_item_mtx, user_similarity = params\n",
    "    test_users = testset_description['test_users']\n",
    "    scores = user_similarity.dot(user_item_mtx)\n",
    "    return scores[test_users, :].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_params_asym = build_uknn_model_asym(\n",
    "    {'weighted': False, 'n_neighbors': n_neighbors}, training, data_description\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_scores_asym = uknn_model_scoring_asym(uknn_params_asym, None, data_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Evaluation (1 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate top-$n$ recommendations for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "downvote_seen_items(uknn_scores_uw, testset, data_description)\n",
    "downvote_seen_items(uknn_scores_asym, testset, data_description)\n",
    "uknn_recs = topn_recommendations(uknn_scores_uw)\n",
    "uknn_recs_asym = topn_recommendations(uknn_scores_asym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity type: symmetric\n",
      "HR=0.085, MRR=0.0286, COV=0.176\n",
      "\n",
      "Similarity type: asymmetric\n",
      "HR=0.0874, MRR=0.0294, COV=0.19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modes = ['symmetric', 'asymmetric']\n",
    "uknn_recs = dict(zip(modes, [uknn_recs, uknn_recs_asym]))\n",
    "\n",
    "\n",
    "uknn_metrics = {}\n",
    "for mode, recs in uknn_recs.items():\n",
    "    if recs is None: continue\n",
    "    uknn_metrics[mode] = metrics = model_evaluate(recs, holdout, data_description)\n",
    "    print(\n",
    "        f'Similarity type: {mode}\\n'\\\n",
    "        'HR={:.3}, MRR={:.3}, COV={:.3}\\n'.format(*metrics)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strong generalization test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall that in the strong generalization test you work with the warm-start scenario.\n",
    "- It means that the set of test users is disjoint from the set of users in the training.\n",
    "- You're provided with the basic functions to help you perform correct splitting, but there're still a few places where your input is required. Make sure you understand the logic of data splitting in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Your task is to select a subset of users who have the most recent interactions in their history across entire dataset.\n",
    "- You will apply holdout splitting to only this subset.\n",
    "  - Think, why simply taking all users (as in weak generalization test) makes no sense in this scenario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_time(data, time_q=0.95, timeid='timestamp'):\n",
    "    '''\n",
    "    Split the input `data` DataFrame into two parts based on the timestamp, with the split point\n",
    "    being determined by the quantile value `time_q`. The function returns a tuple `(before, after)`\n",
    "    containing the two DataFrames. The `after` DataFrame contains the rows with timestamps greater\n",
    "    than or equal to the split point, while the `before` DataFrame contains the remaining rows. \n",
    "\n",
    "    Details:\n",
    "    The `quantile` method of the pandas DataFrame is used to calculate the time point (i.e., timestamp)\n",
    "    that divides the data into two parts based on the given quantile value `time_q`. Specifically,\n",
    "    the time point `split_timepoint` is calculated as the `time_q`th quantile of the values in the `timeid`\n",
    "    column of the `data` DataFrame, using the interpolation method of `nearest`. This means that\n",
    "    `split_timepoint` is the timestamp at or immediately after which `time_q` percent of the data points occur.    \n",
    "    '''\n",
    "    split_timepoint = data[timeid].quantile(q=time_q, interpolation='nearest')\n",
    "    after = data.query(f'{timeid} >= @split_timepoint') \n",
    "    before = data.drop(after.index)\n",
    "    return before, after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, you need to select a candidate subset of observations, from which you'll construct the the training, testset, and holdout datssets. Check the `split_by_time` function below and its description in the above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "before, after = split_by_time(data, time_q=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now it's time to perform holdout sampling based on the obtained timepoint splitting. \n",
    "- Remember, you only sample from the test users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_part_, holdout_ = leave_one_out(\n",
    "    after,          \n",
    "    target='timestamp',\n",
    "    sample_top=True,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "test_ids = testset_part_['userid'].values\n",
    "holdout_ = holdout_[holdout_.userid.isin(test_ids)]\n",
    "\n",
    "# verify correctness of time-based splitting,\n",
    "# i.e., for each test user, the holdout contains only future interactions w.r.t to testset\n",
    "verify_time_split(testset_part_, holdout_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ = before[~before.userid.isin(test_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that `testset_part_` only contains interactions of the test users **after the timepoint**.\n",
    "- You need to combine it with the remaining histories of these users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all test users data into a single `testset_` Dataframe.\n",
    "testset_ = pd.concat(\n",
    "    [before[before.userid.isin(test_ids)], testset_part_],\n",
    "    axis = 0,\n",
    "    ignore_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building internal representation of user and item index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `transform_indices` function for building a contiguous index starting from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, data_index = transform_indices(training_, 'userid', 'movieid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before applying new index to the test data:\n",
    "  - note that the users in the `testset` must be the same as the users in the `holdout`.\n",
    "- Below is the corresponding function `align_test_by_users` that ensures these two datasets' alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_test_by_users(testset, holdout):\n",
    "    test_users = np.intersect1d(holdout['userid'].values, testset['userid'].values)\n",
    "    # only allow the same users to be present in both datasets\n",
    "    testset = testset.query('userid in @test_users').sort_values('userid')\n",
    "    holdout = holdout.query('userid in @test_users').sort_values('userid')\n",
    "    return testset, holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply new item index to test data and finalize the test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 5 invalid observations.\n",
      "Filtered 108 invalid observations.\n"
     ]
    }
   ],
   "source": [
    "holdout = reindex(holdout_, data_index['items'], filter_invalid=True)\n",
    "testset = reindex(testset_, data_index['items'], filter_invalid=True)\n",
    "\n",
    "testset, holdout = align_test_by_users(testset, holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Think why we do not apply new index to users here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section you'll need to implement user-based KNN models for the warm-start scenario.\n",
    "- Think carefully which data must be generated at the build time and which data must be generated in the scoring function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric case (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_uknn_model(config, data, data_description):\n",
    "    user_item_mtx = generate_interactions_matrix(data, data_description)\n",
    "    weighted = config['weighted']\n",
    "    return user_item_mtx, weighted\n",
    "\n",
    "def uknn_model_scoring(params, testset, testset_description):\n",
    "    user_item_mtx = params\n",
    "    user_item_mtx_test = generate_interactions_matrix(testset, testset_description)\n",
    "    print(user_item_mtx_test.shape)\n",
    "    \n",
    "    user_similarity = truncate_similarity(\n",
    "        cosine_similarity_zd(user_item_mtx, user_item_mtx_test),\n",
    "        k = config['n_neighbors']\n",
    "    )\n",
    "    \n",
    "    scores = user_similarity.dot(user_item_mtx)\n",
    "    \n",
    "    if not weighted:\n",
    "        return scores.toarray()\n",
    "    \n",
    "    normalizer = user_similarity.dot(user_item_mtx.astype('bool'))\n",
    "    scores = np.nan_to_num(np.divide(scores, normalizer))\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040 3704\n",
      "(732119,) (732119,)\n"
     ]
    }
   ],
   "source": [
    "uknn_params = build_uknn_model(\n",
    "    {'weighted': False, 'n_neighbors': n_neighbors}, training, data_description\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_zd(m1, m2):\n",
    "    print(m1, m2.shape)\n",
    "    '''Build cosine similarity matrix with zero diagonal.'''\n",
    "    similarity = cosine_similarity(m1, m2, dense_output=False)\n",
    "    similarity.setdiag(0)\n",
    "    similarity.eliminate_zeros()\n",
    "    return similarity.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263087 263087\n",
      "(263087,) (263087,)\n",
      "(<6040x3704 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 732119 stored elements in Compressed Sparse Row format>, False) (263087, 263087)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'csr_matrix'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[203], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m test_description \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m      2\u001b[0m     users \u001b[38;5;241m=\u001b[39m data_index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musers\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m      3\u001b[0m     items \u001b[38;5;241m=\u001b[39m data_index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     n_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(testset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovieid\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m uknn_scores \u001b[38;5;241m=\u001b[39m \u001b[43muknn_model_scoring\u001b[49m\u001b[43m(\u001b[49m\u001b[43muknn_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_description\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[174], line 11\u001b[0m, in \u001b[0;36muknn_model_scoring\u001b[0;34m(params, testset, testset_description)\u001b[0m\n\u001b[1;32m      7\u001b[0m user_item_mtx \u001b[38;5;241m=\u001b[39m params\n\u001b[1;32m      8\u001b[0m user_item_mtx_test \u001b[38;5;241m=\u001b[39m generate_interactions_matrix(testset, testset_description)\n\u001b[1;32m     10\u001b[0m user_similarity \u001b[38;5;241m=\u001b[39m truncate_similarity(\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mcosine_similarity_zd\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_item_mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_item_mtx_test\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     12\u001b[0m     k \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_neighbors\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m scores \u001b[38;5;241m=\u001b[39m user_similarity\u001b[38;5;241m.\u001b[39mdot(user_item_mtx)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m weighted:\n",
      "Cell \u001b[0;32mIn[202], line 4\u001b[0m, in \u001b[0;36mcosine_similarity_zd\u001b[0;34m(m1, m2)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(m1, m2\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Build cosine similarity matrix with zero diagonal.'''\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m similarity \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m similarity\u001b[38;5;241m.\u001b[39msetdiag(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m similarity\u001b[38;5;241m.\u001b[39meliminate_zeros()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:1393\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m \n\u001b[1;32m   1360\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;124;03m    Returns the cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m-> 1393\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1395\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:155\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    146\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m    147\u001b[0m         X,\n\u001b[1;32m    148\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    153\u001b[0m     )\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m    164\u001b[0m         Y,\n\u001b[1;32m    165\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    170\u001b[0m     )\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    883\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    182\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "test_description = dict(\n",
    "    users = data_index['users'].name,\n",
    "    items = data_index['items'].name,\n",
    "    feedback = 'rating',\n",
    "    n_users = len(testset['userid']),\n",
    "    n_items = len(testset['movieid'])\n",
    ")\n",
    "\n",
    "\n",
    "uknn_scores = uknn_model_scoring(uknn_params, testset, test_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5261,)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.userid.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'users': 'userid',\n",
       " 'items': 'movieid',\n",
       " 'feedback': 'rating',\n",
       " 'n_users': 6040,\n",
       " 'n_items': 3704,\n",
       " 'test_users': array([   0,    1,    2, ..., 6037, 6038, 6039])}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymmetric case (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_uknn_model_asym(config, data, data_description):\n",
    "    \n",
    "    return ...\n",
    "\n",
    "def uknn_model_scoring_asym(params, testset, testset_description):\n",
    "\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_params_asym = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_scores_asym = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Evaluation (1 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate recommendations for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_recs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uknn_recs_asym = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['symmetric', 'asymmetric']\n",
    "uknn_recs = dict(zip(modes, [uknn_recs, None]))\n",
    "\n",
    "\n",
    "uknn_metrics = {}\n",
    "for mode, recs in uknn_recs.items():\n",
    "    if recs is None: continue\n",
    "    uknn_metrics[mode] = metrics = model_evaluate(recs, holdout, data_description)\n",
    "    print(\n",
    "        f'Similarity type: {mode}\\n'\\\n",
    "        'HR={:.3}, MRR={:.3}, COV={:.3}\\n'.format(*metrics)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning (2 pts)\n",
    "- Try to find a neighborhood size that gives you better results.\n",
    "- Perform a simple grid-search experiment and report your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final analysis (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Provide an analysis on which model performs the best and explain why.\n",
    "2. Explain the difference in computational complexity of your models. Consider how the training and the recommendation generation differ for different models in terms of\n",
    "    - the amount of RAM,\n",
    "    - the amount of disk storage,\n",
    "    - the load on CPU.\n",
    "3. How else would you modify the model to improve either the quality of recommendations or computational performance? Describe at least one modification and its envisioned effect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "64cd544b7330e8e73b8689d110cc075e8c836a404445c2b82c04f3ea96ea86ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
